{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4_PartB.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNRAKxqvDsYeQLURiBm6TLI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarmanBhutani/ML_projects/blob/main/Assignment4_PartB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfrgiwUNX3Cb",
        "outputId": "c577ee06-a84a-4c2b-f0cb-65d40d7a9343"
      },
      "source": [
        "pip install contractions"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/04/d5e0bb9f2cef5d15616ebf68087a725c5dbdd71bd422bcfb35d709f98ce7/contractions-0.0.48-py2.py3-none-any.whl\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
            "Collecting anyascii\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/c7/61370d9e3c349478e89a5554c1e5d9658e1e3116cc4f2528f568909ebdf1/anyascii-0.1.7-py3-none-any.whl (260kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 34.8MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/c2/eae730037ae1cbbfaa229d27030d1d5e34a1e41114b21447d1202ae9c220/pyahocorasick-1.4.2.tar.gz (321kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 41.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85393 sha256=02b041b7738864fa34f8bba51f7ebae24d6405579365e50e064c5d61b267d37c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: anyascii, pyahocorasick, textsearch, contractions\n",
            "Successfully installed anyascii-0.1.7 contractions-0.0.48 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSzHt_aaXjmQ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import contractions\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MultiLabelBinarizer\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import seaborn as sns"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Giu6j9o4X0TA"
      },
      "source": [
        "df_reut = pd.read_excel('reuters_news_headlines.xlsx', header=None)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "XcfU7K1UYQtd",
        "outputId": "c083931d-c2f1-4f6d-eca7-971441f11a3d"
      },
      "source": [
        "df_reut.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Headlines</td>\n",
              "      <td>Time</td>\n",
              "      <td>Description</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TikTok considers London and other locations fo...</td>\n",
              "      <td>Jul 18 2020</td>\n",
              "      <td>TikTok has been in discussions with the UK gov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Disney cuts ad spending on Facebook amid growi...</td>\n",
              "      <td>Jul 18 2020</td>\n",
              "      <td>Walt Disney  has become the latest company to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Trail of missing Wirecard executive leads to B...</td>\n",
              "      <td>Jul 18 2020</td>\n",
              "      <td>Former Wirecard  chief operating officer Jan M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Twitter says attackers downloaded data from up...</td>\n",
              "      <td>Jul 18 2020</td>\n",
              "      <td>Twitter Inc said on Saturday that hackers were...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  ...                                                  2\n",
              "0                                          Headlines  ...                                        Description\n",
              "1  TikTok considers London and other locations fo...  ...  TikTok has been in discussions with the UK gov...\n",
              "2  Disney cuts ad spending on Facebook amid growi...  ...  Walt Disney  has become the latest company to ...\n",
              "3  Trail of missing Wirecard executive leads to B...  ...  Former Wirecard  chief operating officer Jan M...\n",
              "4  Twitter says attackers downloaded data from up...  ...  Twitter Inc said on Saturday that hackers were...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv1w9ZLtYVYG",
        "outputId": "f10b68dc-15c7-46bf-9a8b-a60dad619c6a"
      },
      "source": [
        "df_reut[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                Headlines\n",
              "1        TikTok considers London and other locations fo...\n",
              "2        Disney cuts ad spending on Facebook amid growi...\n",
              "3        Trail of missing Wirecard executive leads to B...\n",
              "4        Twitter says attackers downloaded data from up...\n",
              "                               ...                        \n",
              "32766    Malaysia says never hired British data firm at...\n",
              "32767    Prosecutors search Volkswagen headquarters in ...\n",
              "32768     McDonald's sets greenhouse gas reduction targets\n",
              "32769    Pratt & Whitney to deliver spare A320neo engin...\n",
              "32770    UK will always consider ways to improve data l...\n",
              "Name: 0, Length: 32771, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it383HyCYCCa"
      },
      "source": [
        "df_reut[0] = df_reut[0].apply(lambda x: ' '.join(x))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnKNV0dfYhoQ"
      },
      "source": [
        "nlp = en_core_web_sm.load()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUGmA-yfYivQ"
      },
      "source": [
        "labels = []\n",
        "for text in df_reut[0]:\n",
        "    en = nlp(text).ents\n",
        "    labels.append([l.label_ for l in en])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX9XkwAiYliZ"
      },
      "source": [
        "labels[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA0PoSqnZq32"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}